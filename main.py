# -*- coding: utf-8 -*-
"""mnist and anoclass

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e1gHY8oT_g-pGE6laT_3pzs9pw2BwQXg

**Anomaly Infused Classification**

This notebook tests an idea of mine: Can you use anomaly detection to improve a classification. If this works (and it seems like it does, currently at ~2 sigma), you can improve any classification network (at least like here with some symmetry in the data (aka convolutions, gnns), but I guess this should also work without) without any other input.

To do this, I use an anomaly detection algorithm I invented (and call oneoff networks), to define anomality for each point on the image. Afterwards I just add this anomality to the input of a convolutional network and compare the result to the same network trained the same way, but without a second input.

The first part is the comparison network, so just a copy of https://keras.io/examples/vision/mnist_convnet/

I dont change much, as I guess you can assume that the usual keras tutorial is well optimized. This suggests that the final result (anomaly infused network) is not as good as it could be.
"""

import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print("x_train shape:", x_train.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation="softmax"),
    ]
)

model.summary()

batch_size = 128
epochs = 15

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

hn=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

score = model.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""**now for the oneoff network**
here I train an image to image network resulting in 1 while removing biases
"""

modeloo = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(7, 7), activation="relu",padding="same",use_bias=False),
        #layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(4, 4), activation="relu",padding="same",use_bias=False),
        layers.Conv2D(16, kernel_size=(4, 4), activation="relu",padding="same",use_bias=False),
        layers.Conv2D(4, kernel_size=(2, 2), activation="relu",padding="same",use_bias=False),
        #layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(1,kernel_size=(1,1),activation="linear",padding="same",use_bias=False),
    ]
)

modeloo.summary()

batch_size = 128
epochs = 5

modeloo.compile(loss="mse", optimizer="adam", metrics=[])

modeloo.fit(x_train, np.ones_like(x_train), batch_size=batch_size, epochs=epochs, validation_split=0.1)

oo=modeloo.predict(x_train)
oot=modeloo.predict(x_test)
print(oo.shape)
of=oo.flatten()
print(of.shape)

import matplotlib.pyplot as plt
plt.hist(of,bins=50)
plt.show()

delta=np.abs(of-1)
plt.hist(delta,bins=50)
plt.yscale("log",nonposy="clip")
plt.show()

#some statistics processing
print(np.mean(delta))
print(np.std(delta))
print(np.max(delta))

stat_mean=np.mean(delta)
stat_sigma=np.std(delta)

def corr(q):
  return np.exp(0.5*(np.abs(q-1)-stat_mean)/(stat_sigma))

print(corr(np.max(delta+1)))
print(corr(1))

oc=corr(oo)
oct=corr(oot)
oct-=np.mean(oc)
oct/=np.std(oct)
oc-=np.mean(oc)
oc/=np.std(oc)

print(oc.shape)
print(x_train.shape)

print(np.mean(oct))
print(np.std(oct))

"""**Now lets use this for anomaly infused training**"""

#at first concat this as new inputs
cc=np.concatenate((x_train,oc),axis=-1)
print(cc.shape)
cct=np.concatenate((x_test,oct),axis=-1)
print(cct.shape)

modelc = keras.Sequential(
    [
        keras.Input(shape=cc.shape[1:]),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation="softmax"),
    ]
)

modelc.summary()

batch_size = 128
epochs = 15

modelc.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

hc=modelc.fit(cc, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

scorec = modelc.evaluate(cct, y_test, verbose=0)
print("Test loss:", scorec[0])
print("Test accuracy:", scorec[1])

acn=hn.history["accuracy"]
acc=hc.history["accuracy"]
print(acn,acc)

"""sadly the images are not very decisive"""

plt.plot(acn,label="normal training")
plt.plot(acc,label="anomaly infused")
plt.legend()
plt.show()

plt.plot([a-b for a,b in zip(acn,acc)])
plt.show()

lon=hn.history["loss"]
loc=hc.history["loss"]
print(lon,loc)

plt.plot(lon,label="normal training",alpha=0.5)
plt.plot(loc,label="anomaly infused",alpha=0.5)
plt.legend()
plt.yscale("log")
plt.show()

"""Quick compare:"""

print("without",score)
print("with   ",scorec)

print("both should be positive")
print("loss",score[0]-scorec[0])
print("acc",scorec[1]-score[1])

"""Rerunning results in

both should be positive
loss -0.0016080308705568314
acc 0.00010001659393310547

both should be positive
loss 0.00017721019685268402
acc -0.000299990177154541

both should be positive
loss 0.005704481154680252
acc 0.00279998779296875

both should be positive
loss 0.0008180048316717148
acc 0.000599980354309082

both should be positive
loss 0.0008532833307981491
acc -0.00010001659393310547

both should be positive
loss 0.0019453037530183792
acc 0.0006000399589538574

both should be positive
loss 0.005755022168159485
acc 0.002200007438659668

both should be positive
loss 3.5393983125686646e-05
acc 0.00019997358322143555

both should be positive
loss -0.0015629995614290237
acc -0.000800013542175293

14/20 >0, sigma~2, so about 2 sigma significance

Having done this quick and dirty, lets do this more proffesional:
"""

ll=[[0.02173512801527977,0.0233431588858366],
    [0.023866165429353714,0.02368895523250103],
    [0.0278194360435009,0.022114954888820648],
    [0.023399805650115013,0.0225818008184433],
    [0.025487083941698074,0.024633800610899925],
    [0.02620108425617218,0.0242557805031538],
    [0.027982046827673912,0.022227024659514427],
    [0.026208026334643364,0.026172632351517677],
    [0.023979149758815765,0.02554214932024479],
    [0.026425588876008987,0.022080114111304283]]

"""I look only at losses, because there less random. First collumn: without, second with"""

fr=len([1 for zw in ll if zw[0]>zw[1]])/len(ll)
print("fraction of improvements",fr)

ln=[zw[0] for zw in ll]
lc=[zw[1] for zw in ll]

import numpy as np
mn=np.mean(ln)
mc=np.mean(lc)
sn=np.std(ln)/np.sqrt(len(ln))
sc=np.std(lc)/np.sqrt(len(lc))

diff=mn-mc
sigma=np.sqrt(sn**2+sc**2)
print("Difference",diff)
print("With error",sigma)
print("This means a significance of")
print(diff/sigma)

"""So still about a 2 sigma improvement"""

